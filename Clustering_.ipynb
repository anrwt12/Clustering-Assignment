{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Question 1: What is the difference between K-Means and Hierarchical Clustering? Provide a use case for each.\n",
        "\n",
        "Answer:\n",
        "\n",
        "K-Means Clustering is a partition-based clustering algorithm. It divides the dataset into K fixed clusters, where each data point belongs to the nearest cluster center (centroid). It works best when clusters are spherical, well-separated, and the number of clusters is already known.\n",
        "\n",
        "Hierarchical Clustering builds clusters in a tree-like structure (dendrogram). It does not require specifying the number of clusters initially. It can be Agglomerative (bottom-up) or Divisive (top-down). It is useful when we want to understand cluster relationships.\n",
        "\n",
        "##Key Differences\n",
        "\n",
        " * K-Means needs K in advance; Hierarchical does not.\n",
        "\n",
        " * K-Means is faster for large datasets; Hierarchical is slower.\n",
        "\n",
        "* K-Means works better for round clusters; Hierarchical can capture complex patterns.\n",
        "\n",
        "##Use Case\n",
        "\n",
        "* K-Means Use Case: Customer segmentation in retail where clusters are well separated (e.g., low spenders, medium spenders, high spenders).\n",
        "\n",
        "* Hierarchical Use Case: Grouping documents/news articles based on similarity to explore topic hierarchy."
      ],
      "metadata": {
        "id": "BcrliY3MpQOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "___\n",
        "#Question 2: Explain the purpose of the Silhouette Score in evaluating clustering algorithms.\n",
        "\n",
        "Answer:\n",
        "The Silhouette Score is used to evaluate how well clustering has been performed. It measures how similar a point is to its own cluster compared to other clusters.\n",
        "\n",
        "* A high silhouette score means the data point is well matched to its own cluster and far from other clusters.\n",
        "\n",
        "* A low score means the point may be assigned to the wrong cluster.\n",
        "\n",
        "* A negative score indicates the point is likely in the wrong cluster.\n",
        "\n",
        "Silhouette score ranges from -1 to +1:\n",
        "\n",
        "* +1 → perfect clustering\n",
        "\n",
        "* 0 → overlapping clusters\n",
        "\n",
        "* -1 → incorrect clustering\n",
        "\n",
        "It helps in selecting the best clustering model and also helps to choose the optimal number of clusters.\n",
        "\n",
        "___\n",
        "\n",
        "\n",
        "##Question 3: What are the core parameters of DBSCAN, and how do they influence the clustering process?\n",
        "\n",
        "Answer:\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) forms clusters based on density.\n",
        "\n",
        "**Core Parameters**\n",
        "\n",
        "**1 eps (epsilon):**\n",
        "It defines the maximum distance between two points to be considered neighbors.\n",
        "\n",
        "* Small eps → too many points become noise\n",
        "\n",
        "* Large eps → clusters merge together\n",
        "\n",
        "**2  min_samples:**\n",
        "It defines the minimum number of points required to form a dense region (core point).\n",
        "\n",
        "* High min_samples → fewer clusters, more noise\n",
        "\n",
        "* Low min_samples → more clusters, less noise\n",
        "\n",
        "**Influence on clustering**\n",
        "\n",
        "* DBSCAN identifies core points, border points, and noise points.\n",
        "\n",
        "* It is useful for detecting clusters of arbitrary shapes and also identifies outliers naturally.\n",
        "\n",
        "\n",
        "\n",
        "____\n",
        "\n",
        "##Question 4: Why is feature scaling important when applying clustering algorithms like K-Means and DBSCAN?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Feature scaling is important because clustering algorithms like K-Means and DBSCAN use distance calculations (usually Euclidean distance).\n",
        "\n",
        "If one feature has a larger range (example: income 0–1,00,000) and another has a smaller range (example: age 0–60), then income will dominate the distance calculation.\n",
        "\n",
        "**Why scaling matters**\n",
        "\n",
        "* It ensures all features contribute equally\n",
        "\n",
        "* It improves cluster accuracy\n",
        "\n",
        "* It prevents bias due to large-scale features\n",
        "\n",
        "* It improves DBSCAN’s eps selection and neighbor detection\n",
        "\n",
        "**Common scaling methods:**\n",
        "\n",
        "* StandardScaler (mean=0, std=1)\n",
        "\n",
        "* MinMaxScaler (0 to 1)\n",
        "\n",
        "____\n",
        "\n",
        "\n",
        "\n",
        "#Question 5: What is the Elbow Method in K-Means clustering and how does it help determine the optimal number of clusters?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Elbow Method is used to find the best value of K in K-Means clustering.\n",
        "\n",
        "It plots:\n",
        "\n",
        "* K (number of clusters) on x-axis\n",
        "\n",
        "* WCSS (Within Cluster Sum of Squares) on y-axis\n",
        "\n",
        "WCSS decreases as K increases, but after a certain point, the improvement becomes very small. That point forms a bend like an elbow.\n",
        "\n",
        "**How it helps**\n",
        "\n",
        "* The “elbow point” gives the optimal K\n",
        "\n",
        "* It avoids selecting too many clusters\n",
        "\n",
        "* It ensures good clustering with minimum complexity"
      ],
      "metadata": {
        "id": "glj66VtYp_Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Generate synthetic data using make_blobs(n_samples=300, centers=4), apply KMeans clustering, and visualize the results with cluster centers. (Include your Python code and output in the code box below.)\n",
        "\n",
        "#Answer:\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            c='red', s=200, marker='X', label='Centers')\n",
        "plt.title(\"KMeans Clustering on make_blobs Data\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zZHq049Es96E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Load the Wine dataset, apply StandardScaler , and then train a DBSCAN model. Print the number of clusters found (excluding noise). (Include your Python code and output in the code box below.)\n",
        "\n",
        "#Answer:\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train DBSCAN\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Count clusters excluding noise (-1)\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters found (excluding noise):\", n_clusters)\n"
      ],
      "metadata": {
        "id": "305QGoiGtHTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Generate moon-shaped synthetic data using make_moons(n_samples=200, noise=0.1), apply DBSCAN, and highlight the outliers in the plot. (Include your Python code and output in the code box below.)\n",
        "\n",
        "#Answer:\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate moon-shaped data\n",
        "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40)\n",
        "\n",
        "# Highlight outliers (noise points)\n",
        "outliers = labels == -1\n",
        "plt.scatter(X[outliers, 0], X[outliers, 1], c='yellow', s=80, label='Outliers')\n",
        "\n",
        "plt.title(\"DBSCAN on make_moons Data with Outliers\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "voHjSemstPom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Load the Wine dataset, reduce it to 2D using PCA, then apply Agglomerative Clustering and visualize the result in 2D with a scatter plot. (Include your Python code and output in the code box below.)\n",
        "\n",
        "#Answer:\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=40)\n",
        "plt.title(\"Agglomerative Clustering on Wine Dataset (PCA Reduced)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sDI-SwNutm8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working as a data analyst at an e-commerce company. The marketing team wants to segment customers based on their purchasing behavior to run targeted promotions. The dataset contains customer demographics and their product purchase history across categories. Describe your real-world data science workflow using clustering: ● Which clustering algorithm(s) would you use and why? ● How would you preprocess the data (missing values, scaling)? ● How would you determine the number of clusters? ● How would the marketing team benefit from your clustering analysis? (Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "#Workflow\n",
        "\n",
        " **1  Business Understanding**\n",
        "\n",
        " The goal is to group customers into meaningful segments based on purchase behavior and demographics.\n",
        "\n",
        "**2  Data Cleaning & Preprocessing**\n",
        "\n",
        "* Handle missing values using:*\n",
        "\n",
        "* mean/median for numerical data\n",
        "\n",
        "* mode for categorical data\n",
        "\n",
        "*  Convert categorical variables using OneHotEncoding\n",
        "\n",
        "* Scale numerical features using StandardScaler (important for distance-based clustering)\n",
        "\n",
        "**3  Algorithm Selection**\n",
        "\n",
        "* K-Means: good for large customer datasets, fast, easy to interpret\n",
        "\n",
        "* DBSCAN: useful for detecting outliers (fraud/unusual customers)\n",
        "\n",
        "* For this use case, I will mainly use K-Means for segmentation.\n",
        "\n",
        "**4  Selecting number of clusters**\n",
        "\n",
        "* Use Elbow Method*\n",
        "\n",
        "* Use Silhouette Score to confirm best clustering quality\n",
        "\n",
        "**Marketing Benefits**\n",
        "\n",
        "* Helps create personalized campaigns\n",
        "\n",
        "* Identify high-value customers for premium offers\n",
        "\n",
        "* Identify discount seekers and offer coupons\n",
        "\n",
        "* Improve customer retention and increase revenue"
      ],
      "metadata": {
        "id": "f1P7ztsut1lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Python Code Example\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Example dataset (dummy customer purchase behavior)\n",
        "data = {\n",
        "    \"age\": [22, 25, 45, 35, 52, 23, 40, 60],\n",
        "    \"income\": [25000, 30000, 80000, 60000, 90000, 28000, 65000, 100000],\n",
        "    \"electronics_spend\": [2000, 2500, 12000, 9000, 15000, 1800, 9500, 16000],\n",
        "    \"grocery_spend\": [5000, 4500, 3000, 3500, 2000, 5200, 3200, 1800]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df)\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Evaluate\n",
        "sil_score = silhouette_score(X_scaled, labels)\n",
        "\n",
        "print(\"Cluster Labels:\", labels)\n",
        "print(\"Silhouette Score:\", sil_score)\n"
      ],
      "metadata": {
        "id": "VCJcEvWRvJ_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}